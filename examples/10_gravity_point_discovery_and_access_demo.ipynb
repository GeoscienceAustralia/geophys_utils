{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up search criteria\n",
    "GDA94_bounds = (119, -28.25, 119.75, -27.5) # Bounding box in GDA94 coordinates\n",
    "keywords = 'point, gravity, point located data, ground digital data, geophysical survey' # Comma-separated list of keywords\n",
    "grid_variable_name = 'bouguer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from geophys_utils import NetCDFPointUtils\n",
    "import matplotlib.pyplot as plt\n",
    "from geophys_utils import array2file\n",
    "from osgeo import gdal\n",
    "from geophys_utils import CSWUtils\n",
    "import os\n",
    "import re\n",
    "from netCDF4 import Dataset\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import math\n",
    "from scipy.interpolate import griddata\n",
    "from skimage import exposure\n",
    "from geophys_utils import transform_coords, get_utm_wkt, get_spatial_ref_from_wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netcdf_datasets(keywords, \n",
    "                        bounding_box=None, \n",
    "                        start_date_string=None, \n",
    "                        end_date_string=None, \n",
    "                        csw_url=None,\n",
    "                        ):\n",
    "    '''\n",
    "    Find all datasets of interest and return a list of NetCDF file paths or OPeNDAP web service endpoints\n",
    "    '''    \n",
    "    csw_url = csw_url or 'https://ecat.ga.gov.au/geonetwork/srv/eng/csw'\n",
    "    #create a csw_utils object and populate the parameters with search parameters\n",
    "    # N.B: \"verify\" parameter requires hack to geophys_utils.csw_utils, owslib.csw & owslib.utils \n",
    "    try:\n",
    "        cswu = CSWUtils(csw_url) \n",
    "    except:\n",
    "        cswu = CSWUtils(csw_url, verify=False) \n",
    "        \n",
    "    if start_date_string:\n",
    "        start_datetime = date_string2datetime(start_date_string)\n",
    "        assert start_datetime is not None, 'Invalid date string for start date'\n",
    "    else:\n",
    "        start_datetime = None\n",
    "        \n",
    "    if end_date_string:\n",
    "        end_datetime = date_string2datetime(end_date_string)\n",
    "        assert end_datetime is not None, 'Invalid date string for end date'\n",
    "    else:\n",
    "        end_datetime = None\n",
    "        \n",
    "    print('Querying CSW')\n",
    "    record_list = [record for record in cswu.query_csw(keyword_list=keywords,\n",
    "                                      #anytext_list=allwords,\n",
    "                                      #titleword_list=titlewords,\n",
    "                                      bounding_box=bounding_box,\n",
    "                                      start_datetime=start_datetime,\n",
    "                                      stop_datetime=end_datetime,\n",
    "                                      #max_total_records=2000\n",
    "                                      )\n",
    "              ]\n",
    "    print('{} matching dataset records found from CSW'.format(len(record_list)))\n",
    "    \n",
    "    netcdf_list = [distribution['url']\n",
    "            for distribution in cswu.get_netcdf_urls(record_list)\n",
    "            ]\n",
    "\n",
    "    print('{} NetCDF distributions found'.format(len(netcdf_list)))\n",
    "    \n",
    "    return netcdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_value_generator(variable_name_list,\n",
    "                            dataset_list, \n",
    "                            bounding_box, \n",
    "                            coordinate_wkt,\n",
    "                            min_points=None,\n",
    "                            max_points=None\n",
    "                            ):\n",
    "    '''\n",
    "    Generator yielding coordinates and values of the specified variable for all points from the supplied dataset list \n",
    "    which fall within bounds\n",
    "    '''    \n",
    "    line_dataset_count = 0\n",
    "    for dataset in dataset_list:\n",
    "        line_data = {}\n",
    "        print('Reading values from point dataset {}'.format(dataset))\n",
    "        if True:#try:\n",
    "            nc_dataset = Dataset(dataset)\n",
    "            netcdf_point_utils = NetCDFPointUtils(nc_dataset) \n",
    "            \n",
    "            #print netcdf_point_utils.__dict__\n",
    "            #print(nc_dataset.variables.keys())\n",
    "            \n",
    "            #print('Computing spatial mask')\n",
    "            spatial_mask = netcdf_point_utils.get_spatial_mask(bounding_box)\n",
    "            \n",
    "            if not point_count:\n",
    "                #print('Skipping dataset with no points in bounding box')\n",
    "                continue\n",
    "            \n",
    "            # Enforce min/max point counts\n",
    "            if min_points and point_count < min_points:\n",
    "                print('Skipping dataset with < {} points'.format(min_points))\n",
    "                continue\n",
    "            if max_points and point_count > max_points:\n",
    "                print('Skipping dataset with > {} points'.format(max_points))\n",
    "                continue\n",
    "                \n",
    "            point_count = np.count_nonzero(spatial_mask)\n",
    "            print('{}/{} points found in bounding box for {}'.format(point_count, netcdf_point_utils.point_count, dataset))\n",
    "                        \n",
    "            dataset_value_dict = {'coordinates': netcdf_point_utils.xycoords[spatial_mask]}\n",
    "            \n",
    "            # Read all variable attributes and values\n",
    "            for variable_name in variable_name_list:\n",
    "                variable = nc_dataset.variables[variable_name]\n",
    "                if (variable.dimensions[0] != 'point'): # Variable is NOT of point dimension - must be lookup\n",
    "                    dataset_value_dict[variable_name] = netcdf_point_utils.expand_lookup_variable(lookup_variable_name=variable_name, \n",
    "                                                                                                  mask=spatial_mask)                     \n",
    "                else: # 'point' is in variable.dimensions - \"normal\" variable                \n",
    "                    dataset_value_dict[variable_name] = variable[spatial_mask]\n",
    "            \n",
    "            yield dataset, dataset_value_dict\n",
    "    \n",
    "        else:#except Exception as e:\n",
    "            print('Unable to read point dataset {}: {}'.format(dataset, e))\n",
    "        if False:#finally:\n",
    "            del netcdf_point_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_points(coordinates,\n",
    "                coordinate_wkt,\n",
    "                values,\n",
    "                grid_wkt, \n",
    "                grid_bounds,\n",
    "                grid_resolution, \n",
    "                resampling_method='linear', \n",
    "                point_step=1):\n",
    "    '''\n",
    "    Return interpolated grid from supplied coordinates and points\n",
    "    '''\n",
    "    \n",
    "    # Determine spatial grid bounds rounded out to nearest GRID_RESOLUTION multiple\n",
    "    pixel_centre_bounds = (round(math.floor(grid_bounds[0] / grid_resolution) * grid_resolution, 6),\n",
    "                   round(math.floor(grid_bounds[1] / grid_resolution) * grid_resolution, 6),\n",
    "                   round(math.floor(grid_bounds[2] / grid_resolution - 1.0) * grid_resolution + grid_resolution, 6),\n",
    "                   round(math.floor(grid_bounds[3] / grid_resolution - 1.0) * grid_resolution + grid_resolution, 6)\n",
    "                   )\n",
    "    \n",
    "    print(\"Reprojecting coordinates\")\n",
    "    grid_coordinates = np.array(transform_coords(coordinates, coordinate_wkt, grid_wkt))\n",
    "\n",
    "    print(\"Computing spatial mask\")\n",
    "    spatial_subset_mask = np.logical_and(np.logical_and((grid_bounds[0] <= grid_coordinates[:,0]), \n",
    "                                                        (grid_coordinates[:,0] <= grid_bounds[2])), \n",
    "                                         np.logical_and((grid_bounds[1] <= grid_coordinates[:,1]), \n",
    "                                                        (grid_coordinates[:,1] <= grid_bounds[3]))\n",
    "                                        )    \n",
    "    # Create grids of Y and X values. Note YX ordering and inverted Y for image\n",
    "    # Note GRID_RESOLUTION/2.0 fudge to avoid truncation due to rounding error\n",
    "    print(\"Generating grid coordinates\")\n",
    "    grid_y, grid_x = np.mgrid[pixel_centre_bounds[3]:pixel_centre_bounds[1]-grid_resolution/2.0:-grid_resolution, \n",
    "                              pixel_centre_bounds[0]:pixel_centre_bounds[2]+grid_resolution/2.0:grid_resolution]\n",
    "\n",
    "    # Skip points to reduce memory requirements\n",
    "    print(\"Generating spatial subset mask\")\n",
    "    point_subset_mask = np.zeros(shape=values.shape, dtype=bool)\n",
    "    point_subset_mask[0:-1:point_step] = True\n",
    "    point_subset_mask = np.logical_and(spatial_subset_mask, point_subset_mask)\n",
    "    assert point_subset_mask.any(), 'No points found within grid bounds %s' % grid_bounds\n",
    "    \n",
    "    grid_coordinates = grid_coordinates[point_subset_mask]\n",
    "\n",
    "    # Interpolate required values to the grid - Note yx ordering for image\n",
    "    print(\"Interpolating {} points\".format(grid_coordinates.shape[0]))\n",
    "    grid_array = griddata(grid_coordinates[:,::-1],\n",
    "                          values[point_subset_mask],\n",
    "                          (grid_y, grid_x), \n",
    "                          method=resampling_method\n",
    "                          )\n",
    "\n",
    "    print(\"Interpolation complete\")\n",
    "    #  crs:GeoTransform = \"109.1002342895272 0.00833333 0 -9.354948067227777 0 -0.00833333 \"\n",
    "    geotransform = [pixel_centre_bounds[0]-grid_resolution/2.0,\n",
    "                    grid_resolution,\n",
    "                    0,\n",
    "                    pixel_centre_bounds[3]+grid_resolution/2.0,\n",
    "                    0,\n",
    "                    -grid_resolution\n",
    "                    ] \n",
    "\n",
    "    return grid_array, grid_wkt, geotransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying CSW\n",
      "23 matching dataset records found from CSW\n",
      "23 NetCDF distributions found\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P195099/points/P195099_GNDGRAV/P195099_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P196091/points/P196091_GNDGRAV/P196091_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P196491/points/P196491_GNDGRAV/P196491_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P196500/points/P196500_GNDGRAV/P196500_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P196792/points/P196792_GNDGRAV/P196792_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P196793/points/P196793_GNDGRAV/P196793_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P198090/points/P198090_GNDGRAV/P198090_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P199191/points/P199191_GNDGRAV/P199191_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P200799/points/P200799_GNDGRAV/P200799_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P201099/points/P201099_GNDGRAV/P201099_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P201299/points/P201299_GNDGRAV/P201299_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P201590/points/P201590_GNDGRAV/P201590_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P201691/points/P201691_GNDGRAV/P201691_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P197100/points/P197100_GNDGRAV/P197100_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P197102/points/P197102_GNDGRAV/P197102_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P197103/points/P197103_GNDGRAV/P197103_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P197104/points/P197104_GNDGRAV/P197104_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P200287/points/P200287_GNDGRAV/P200287_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P200861/points/P200861_GNDGRAV/P200861_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P201063/points/P201063_GNDGRAV/P201063_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P201065/points/P201065_GNDGRAV/P201065_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P201260/points/P201260_GNDGRAV/P201260_GNDGRAV.nc\n",
      "http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/WA/P201561/points/P201561_GNDGRAV/P201561_GNDGRAV.nc\n"
     ]
    }
   ],
   "source": [
    "# Find all NetCDF paths using CSW\n",
    "netcdf_list = sorted(get_netcdf_datasets(keywords, \n",
    "                                         bounding_box=GDA94_bounds))\n",
    "print('\\n'.join(netcdf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate extra spatial information about area of interest\n",
    "GDA94_wkt = get_spatial_ref_from_wkt('EPSG:4283').ExportToWkt()\n",
    "\n",
    "GDA94_centre_coords = [(GDA94_bounds[dim_index] + GDA94_bounds[dim_index+2]) / 2.0 for dim_index in range(2)]\n",
    "\n",
    "utm_wkt = get_utm_wkt(GDA94_centre_coords, GDA94_wkt)\n",
    "\n",
    "reprojected_bounding_box = np.array(transform_coords(((GDA94_bounds[0], GDA94_bounds[1]), \n",
    "                                                      (GDA94_bounds[2], GDA94_bounds[1]), \n",
    "                                                      (GDA94_bounds[2], GDA94_bounds[3]), \n",
    "                                                      (GDA94_bounds[0], GDA94_bounds[3])\n",
    "                                                      ), \n",
    "                                                      GDA94_wkt, utm_wkt))\n",
    "utm_bounds = [min(reprojected_bounding_box[:,0]), \n",
    "              min(reprojected_bounding_box[:,1]), \n",
    "              max(reprojected_bounding_box[:,0]), \n",
    "              max(reprojected_bounding_box[:,1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading values from point dataset http://dapds00.nci.org.au/thredds/dodsC/rr2/ground_gravity/Commonwealth/P195099/points/P195099_GNDGRAV/P195099_GNDGRAV.nc\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'point_count' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a012ab888e38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                               \u001b[0mnetcdf_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                               \u001b[0mGDA94_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                                               \u001b[0mutm_wkt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                                               )\n\u001b[0;32m     10\u001b[0m                  }\n",
      "\u001b[1;32m<ipython-input-9-a012ab888e38>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# N.B: This may take some time depending on the size of the bounds and the number and size of line datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Lines will be filtered to exclude tielines by using \"flight_lines_only=True\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m dataset_values = {dataset: dataset_value_dict\n\u001b[0m\u001b[0;32m      5\u001b[0m                   for dataset, dataset_value_dict in dataset_value_generator([grid_variable_name, 'gridflag'],\n\u001b[0;32m      6\u001b[0m                                                               \u001b[0mnetcdf_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a950d983d522>\u001b[0m in \u001b[0;36mdataset_value_generator\u001b[1;34m(variable_name_list, dataset_list, bounding_box, coordinate_wkt, min_points, max_points)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mspatial_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetcdf_point_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_spatial_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpoint_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[1;31m#print('Skipping dataset with no points in bounding box')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'point_count' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Retrieve all points in bounding box from NetCDF files in UTM projection\n",
    "# N.B: This may take some time depending on the size of the bounds and the number and size of line datasets\n",
    "# Lines will be filtered to exclude tielines by using \"flight_lines_only=True\"\n",
    "dataset_values = {dataset: dataset_value_dict\n",
    "                  for dataset, dataset_value_dict in dataset_value_generator([grid_variable_name, 'gridflag'],\n",
    "                                                              netcdf_list, \n",
    "                                                              GDA94_bounds, \n",
    "                                                              utm_wkt,\n",
    "                                                              )\n",
    "                 }\n",
    "\n",
    "#print(dataset_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filtered points for each dataset\n",
    "# Only show points where gridflag == 'Station used in the production of GA grids.'\n",
    "for dataset in sorted(dataset_values.keys()):\n",
    "    #print(dataset)\n",
    "    #print(len(dataset_values[dataset]['coordinates']))\n",
    "    #print(len(dataset_values[dataset]['gridflag'] == 'Station used in the production of GA grids.'))\n",
    "    coordinates = dataset_values[dataset]['coordinates'][dataset_values[dataset]['gridflag'] == 'Station used in the production of GA grids.']\n",
    "    if len(coordinates):\n",
    "        plt.figure(figsize=(30,20))\n",
    "        plt.xlabel(os.path.basename(dataset), fontsize=28, color='red')\n",
    "        plt.plot(coordinates[:,0], coordinates[:,1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all filtered points in bounding box in UTM projection\n",
    "# Only use points where gridflag == 'Station used in the production of GA grids.'\n",
    "all_coordinates = np.concatenate([dataset_values[dataset]['coordinates'][dataset_values[dataset]['gridflag'] == 'Station used in the production of GA grids.']\n",
    "                                  for dataset in sorted(dataset_values.keys())])\n",
    "all_values = np.concatenate([dataset_values[dataset][grid_variable_name][dataset_values[dataset]['gridflag'] == 'Station used in the production of GA grids.']\n",
    "                                  for dataset in sorted(dataset_values.keys())])\n",
    "print(all_coordinates)\n",
    "print(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine grid resolution dynamically\n",
    "grid_area = (utm_bounds[2] - utm_bounds[0]) * (utm_bounds[3] - utm_bounds[1]) # Area of bounding box in square metres\n",
    "grid_resolution = math.sqrt(grid_area / len(all_coordinates)) # Determine square root of (points / square metre)\n",
    "grid_resolution = math.pow(10.0, math.floor(math.log10(grid_resolution))) # Round pixel size down to nearest power of ten\n",
    "print(grid_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate points to grid\n",
    "interpolated_grid, wkt, geotransform = grid_points(coordinates=all_coordinates,\n",
    "                                 coordinate_wkt=GDA94_wkt,\n",
    "                                 values=all_values,\n",
    "                                 grid_wkt=utm_wkt, \n",
    "                                 grid_bounds=utm_bounds,\n",
    "                                 grid_resolution=grid_resolution, \n",
    "                                 resampling_method='linear', \n",
    "                                 point_step=1\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some stats from the grid\n",
    "print('Shape: ', interpolated_grid.shape)\n",
    "print('Min:  ', np.nanmin(interpolated_grid))\n",
    "print('Mean: ', np.nanmean(interpolated_grid))\n",
    "print('Max:  ', np.nanmax(interpolated_grid))\n",
    "print('geotransform: ', geotransform)\n",
    "print('wkt: ', wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interpolated grid with histogram equalisation\n",
    "# Substitute minimum value for NaN values\n",
    "filled_grid = np.array(interpolated_grid)\n",
    "filled_grid[np.isnan(filled_grid)] = np.nanmin(filled_grid)\n",
    "plt.figure(figsize=(30,20))    \n",
    "plt.imshow(exposure.equalize_hist(filled_grid), cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the array to a GeoTIFF file on disk\n",
    "gdal_dataset = array2file([interpolated_grid], wkt, geotransform, 'interpolated_grid.tif', 'GTiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and replot image from GeoTIFF file as a test\n",
    "del gdal_dataset\n",
    "gdal_dataset = gdal.Open('interpolated_grid.tif')\n",
    "plt.figure(figsize=(30,20))    \n",
    "plt.imshow(gdal_dataset.GetRasterBand(1).ReadAsArray())\n",
    "del gdal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
